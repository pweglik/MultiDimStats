---
title: "Drzewa decyzyjne i modele pochodne"
date: "Semestr letni 2021/22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
library(ISLR)
library(MASS)
library(tree)
library(randomForest)
library(gbm)
```

## Drzewa decyzyjne

Drzewa decyzyjne są zaimplementowane w pakiecie `tree` (nieco odmienna
implementacja dostępna jest w pakiecie `rpart`).

### Drzewa klasyfikacyjne

Poniższy kod wykorzystuje zbiór danych `Carseats` z pakietu `ISLR`. Będziemy
klasyfikować obserwacje do dwóch klas: *wysoka sprzedaż* i *niska sprzedaż*.
Uzupełniamy zbiór danych
```{r CarseatsDS}
High <- factor(ifelse(Carseats$Sales <= 8, "No", "Yes"))
CarseatsH <- data.frame(Carseats, High)
```
To samo inaczej
```{r within}
CarseatsH <- within(Carseats, High <- factor(ifelse(Sales <= 8, "No", "Yes")))
```
i jeszcze inaczej
```{r transform}
CarseatsH <- transform(Carseats, High = factor(ifelse(Sales <= 8, "No", "Yes")))
```

Budujemy drzewo klasyfikacyjne do predykcji `High` na podstawie pozostałych
zmiennych (poza `Sales`).

```{r classTree}
sales_high_tree <- tree(High ~ . - Sales, data = CarseatsH)
summary(sales_high_tree)
```

Dla drzew klasyfikacyjnych
$$
  \text{deviance} = -2 n \sum_{m=1}^{|T|} \sum_{k=1}^K \hat{p}_{mk} \log \hat{p}_{mk}
$$
oraz
$$
  \text{residual mean deviance} = \frac{\text{deviance}}{n - |T|}.
$$

Przedstawienie graficzne dopasowanego modelu
```{r plottree}
plot(sales_high_tree)
text(sales_high_tree, pretty = 0)
```

Więcej informacji podaje funkcja `print.tree()`
```{r print_tree}
sales_high_tree
```

[**Które predyktory są najbardziej istotne?**]

Metodą zbioru walidacyjnego estymujemy błąd testowy dla drzewa klasyfikacyjnego
w rozważanym problemie.
```{r classtreeerror}
set.seed(1)
n <- nrow(CarseatsH)
train <- sample(n, n / 2)
test <- -train
sales_high_tree <- tree(High ~ . - Sales, data = CarseatsH, subset = train)
tree_class <- predict(sales_high_tree, newdata = CarseatsH[test,], type = "class")
table(tree_class, CarseatsH$High[test])
mean(tree_class != CarseatsH$High[test])
```

*Duże* drzewo $T_0$ dla zbioru uczącego `CarseatsH[train,]`
```{r bigclasstree}
plot(sales_high_tree)
text(sales_high_tree, pretty = 0)
```

Do znalezienia optymalnego poddrzewa stosujemy przycinanie stosowane złożonością.
Przy pomocy CV konstruujemy ciąg poddrzew wyznaczony przez malejącą złożoność.

```{r classtreecv}
set.seed(1)
sales_high_cv <- cv.tree(sales_high_tree, FUN = prune.misclass)
sales_high_cv
plot(sales_high_cv$size, sales_high_cv$dev, type = "b")
```

Składowa `sales_high_cv$dev` zawiera liczbę błędów CV. Przycinamy drzewo $T_0$
do poddrzewa z najmniejszym poziomem błędów CV.

```{r class.tree.prune}
size_opt <- sales_high_cv$size[which.min(sales_high_cv$dev)]
sales_high_pruned <- prune.misclass(sales_high_tree, best = size_opt)
plot(sales_high_pruned)
text(sales_high_pruned, pretty = 0)
```

Testowy poziom błędów dla optymalnego poddrzewa.
```{r class.pruned.error}
pruned_class <- predict(sales_high_pruned, newdata = CarseatsH[test,], 
                        type = "class")
table(pruned_class, CarseatsH$High[test])
mean(pruned_class != CarseatsH$High[test])
```

[**Narysuj wykres błędu testowego w zależności od rozmiaru poddrzewa.**]

### Drzewa regresyjne

Używamy zbioru danych `Boston` z pakietu `MASS`. Konstruujemy drzewo decyzyjne
dla problemu regresji `medv` względem pozostałych zmiennych.

```{r regressiontree}
medv_tree <- tree(medv ~ ., data = Boston)
summary(medv_tree)
```

*Deviance* oznacza tutaj RSS. Przedstawienie drzewa
```{r medvtreeshow}
medv_tree
plot(medv_tree)
text(medv_tree)
```

[**Które predyktory są najistotniejsze?**]

Metodą zbioru walidacyjnego szacujemy błąd testowy.

```{r medvtreeerror}
set.seed(1)
n <- nrow(Boston)
train <- sample(n, n / 2)
test <- -train
medv_tree <- tree(medv ~ ., data = Boston, subset = train)
medv_pred <- predict(medv_tree, newdata = Boston[test,])
mean((medv_pred - Boston$medv[test])^2)
```

Wyznaczamy optymalne poddrzewo metodą przycinania sterowanego złożonością.

```{r medv.tree.cv}
medv_cv <- cv.tree(medv_tree)
plot(medv_cv$size, medv_cv$dev, type = "b")
```

[**Które poddrzewo jest optymalne? Jaki jest jego (estymowany) błąd testowy?**]

Przycinanie drzewa $T_0$ do żądanego poziomu realizuje w tym przypadku funkcja
`prune.tree()`.

```{r medv.prune}
medv_pruned <- prune.tree(medv_tree, best = 4)
plot(medv_pruned)
text(medv_pruned)
```

[**Oblicz estymatę błędu testowego dla poddrzewa z 4 liśćmi.**]

## Bagging i lasy losowe

Bagging i lasy losowe implementowane są przez pakiet `randomForest`.
Oczywiście bagging jest szczególnym przypadkiem lasu losowego.

### Bagging

Bagging dla regresji `medv` względem wszystkich pozostałych w zbiorze `Boston`.

```{r medvbag}
medv_bag <- randomForest(medv ~ ., data = Boston, mtry = 13, importance = TRUE)
medv_bag
```

Wykres błędu OOB względem liczby drzew
```{r medvbagoob}
plot(medv_bag, type = "l")
```
W przypadku regresji błąd MSE OOB dostępny jest w składowej `mse` obiektu
klasy `randomForest`.
W przypadku klasyfikacji wyniki analizy danych OOB dostępne są w składowych 
`err.rate` (proporcja błędów) i `confusion` (tabela pomyłek).

Wyznaczenie ważności predyktorów
```{r medvimportance}
importance(medv_bag)
```
I stosowny obrazek
```{r medvimpplot}
varImpPlot(medv_bag)
```

Oszacowanie błędu testowego dla poprzednio wyznaczonego zbioru walidacyjnego.
```{r medvbagvalid}
set.seed(2)
medv_bag <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 13,
                         importance = TRUE)
medv_pred_bag <- predict(medv_bag, newdata = Boston[test,])
mean((medv_pred_bag - Boston$medv[test])^2)
```

[**Czy dla zmniejszonego zbioru uczącego zmieniła się ważność predyktorów?**]

Powyższe dla mniejszej liczby hodowanych drzew
```{r medvbagvalidsmall}
set.seed(2)
medv_bag_s <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 13,
                         importance = TRUE, ntree = 25)
medv_pred_bag_s <- predict(medv_bag_s, newdata = Boston[test,])
mean((medv_pred_bag_s - Boston$medv[test])^2)
```

### Lasy losowe

Domyślna wartość parametru `mtry` to $\sqrt{p}$ dla regresji i $p/3$ dla 
klasyfikacji.

Oszacowanie błędu testowego dla poprzednio wyznaczonego zbioru walidacyjnego.
```{r medvrfvalid}
set.seed(2)
medv_rf <- randomForest(medv ~ ., data = Boston, subset = train,
                         importance = TRUE)
medv_pred_rf <- predict(medv_rf, newdata = Boston[test,])
mean((medv_pred_rf - Boston$medv[test])^2)
```

[**Co w tym przypadku można powiedzieć o istotności predyktorów?**]

[**Porównaj na wykresie błędy OOB dla baggingu i domyślnie skonfigurowanego
lasu losowego.**]

Powyższe dla ręcznie ustawionego parametru $m$ (czyli `mtry`).
```{r medv.rf.valid.mtry}
set.seed(2)
medv_rf <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 6,
                         importance = TRUE)
medv_pred_rf <- predict(medv_rf, newdata = Boston[test,])
mean((medv_pred_rf - Boston$medv[test])^2)
```

## Boosting

Używamy algorytmów boostingu dla drzew decyzyjnych zaimplementowanych w 
pakiecie `gbm`. Inną implementację --- wydajną i często pojawiającą się
w zastosowaniach --- zawiera pakiet `xgboost`.

Boosting dla regresji `medv` względem pozostałych zmiennych ze zbioru `Boston`.
Funkcją dopasowującą model jest `gbm()` z istotnymi parametrami:

- `distribution`: `"gaussian"` dla regresji z RSS, `"bernoulli"` dla regresji typu
logistycznego;

- `n.trees`: liczba hodowanych drzew ($B$);

- `interaction.depth`: głębokość interakcji ($d$);

- `shrinkage`: parametr spowalniający uczenie ($\lambda$).

```{r boost}
medv_boost <- gbm(medv ~ ., data = Boston, distribution = "gaussian",
                  n.trees = 5000, interaction.depth = 4)
medv_boost
```

Funkcja `summary.gbm()` wyznacza ważność predyktorów i (domyślnie) wykonuje
odpowiedni wykres.
```{r boostimp}
summary(medv_boost)
```

[**Które predyktory teraz są najistotniejsze?**]

Funkcja `plot.gbm()` wykonuje *wykresy częściowej zaleźności*.
```{r medvboostpdp}
plot(medv_boost, i.var = "rm")
plot(medv_boost, i.var = "lstat")
plot(medv_boost, i.var = c("rm", "lstat"))
```

Oszacowanie błędu testowego dla poprzednio wyznaczonego zbioru walidacyjnego.
```{r medvboostvalid}
set.seed(2)
medv_boost <- gbm(medv ~ ., data = Boston[train,], distribution = "gaussian",
                  interaction.depth = 4, n.trees = 5000)
medv_pred_boost <- predict(medv_boost, newdata = Boston[test,], n.trees = 5000)
mean((medv_pred_boost - Boston$medv[test])^2)
```

To samo dla $\lambda = 0.01$.
```{r medvboostvalid2}
set.seed(2)
medv_boost <- gbm(medv ~ ., data = Boston[train,], distribution = "gaussian",
                  interaction.depth = 4, n.trees = 5000, shrinkage = 0.01)
medv_pred_boost <- predict(medv_boost, newdata = Boston[test,], n.trees = 5000)
mean((medv_pred_boost - Boston$medv[test])^2)
```

To samo dla $d = 1$.
```{r medvboostvalid3}
set.seed(2)
medv_boost <- gbm(medv ~ ., data = Boston[train,], distribution = "gaussian",
                  n.trees = 5000, shrinkage = 0.01)
medv_pred_boost <- predict(medv_boost, newdata = Boston[test,], n.trees = 5000)
mean((medv_pred_boost - Boston$medv[test])^2)
```

[**Użyj baggingu, lasów losowych i boostingu do analizy problemu klasyfikacji
sprzedaży w zbiorze `CarseatsH`. Jak zastosowanie tych metod wpływa na
jakość klasyfikacji? Co można powiedzieć o ważności predyktorów?**]

**Uwaga**. Obecna implementacja funkcji `gbm()` nie działa jeśli
zmienna odpowiedzi jest czynnikiem o 2 poziomach. Należy taką zmienną
przekształcić na zmienną numeryczną o wartościach w zbiorze $\{0, 1\}$
lub na zmienną logiczną. Np. w powyższym ćwiczeniu zamiast zmiennej `High`
można użyć `I(High == "Yes")`.
